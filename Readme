from pyspark.sql import SparkSession
from pyspark.sql.functions import lit, when
from pyspark.sql import functions as F

# Initialize Spark Session (Databricks automatically does this)
spark = SparkSession.builder.appName("DynamicPivot").getOrCreate()

# Read data from an existing table
df = spark.sql("SELECT * FROM data_table")

# Show the initial data
df.show()

# Get distinct values of 'd' column
distinct_segment_values = df.select("segment").distinct().rdd.flatMap(lambda x: x).collect()

# Create pivot expressions
pivot_exprs = [
    F.max(when(F.col('segment') == val, lit(1)).otherwise(lit(0))).alias(val)
    for val in distinct_segment_values
]

# Pivot the data
pivoted_df = df.groupBy("ID", "Territory", "Geography").agg(*pivot_exprs)

# Show the pivoted data
pivoted_df.show()
